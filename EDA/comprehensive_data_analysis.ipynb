{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comprehensive Data Analysis - Train and Test Datasets\n",
    "\n",
    "This notebook provides a thorough exploratory data analysis of the train.csv and test.csv datasets, focusing on:\n",
    "- Data structure and basic information\n",
    "- Missing values analysis\n",
    "- Duplicate detection\n",
    "- Statistical summaries\n",
    "- Data visualization\n",
    "- Column-wise analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "import os\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Create graphs directory if it doesn't exist\n",
    "graphs_dir = 'graphs'\n",
    "if not os.path.exists(graphs_dir):\n",
    "    os.makedirs(graphs_dir)\n",
    "    print(f\"Created directory: {graphs_dir}\")\n",
    "else:\n",
    "    print(f\"Directory {graphs_dir} already exists\")\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette('husl')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Initial Inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== TRAIN DATASET ===\n",
      "Shape: (1460, 11)\n",
      "Columns: ['item_id', 'Time', 'temperature_2m', 'relativehumidity_2m', 'dewpoint_2m', 'windspeed_10m', 'windspeed_100m', 'winddirection_10m', 'winddirection_100m', 'windgusts_10m', 'Power']\n",
      "\n",
      "=== TEST DATASET ===\n",
      "Shape: (60, 12)\n",
      "Columns: ['row_id', 'item_id', 'anchor_time', 'Time', 'anchor_temperature_2m', 'anchor_relativehumidity_2m', 'anchor_dewpoint_2m', 'anchor_windspeed_10m', 'anchor_windspeed_100m', 'anchor_winddirection_10m', 'anchor_winddirection_100m', 'anchor_windgusts_10m']\n"
     ]
    }
   ],
   "source": [
    "# Load the datasets\n",
    "train_df = pd.read_csv('../data/train.csv')\n",
    "test_df = pd.read_csv('../data/test.csv')\n",
    "\n",
    "print(\"=== TRAIN DATASET ===\")\n",
    "print(f\"Shape: {train_df.shape}\")\n",
    "print(f\"Columns: {list(train_df.columns)}\")\n",
    "print(\"\\n=== TEST DATASET ===\")\n",
    "print(f\"Shape: {test_df.shape}\")\n",
    "print(f\"Columns: {list(test_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Analysis - Train Dataset Focus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Focus on train dataset for detailed analysis\n",
    "dataset_df = train_df.copy()\n",
    "\n",
    "# Analyze dataset\n",
    "dataset_head = dataset_df.head().to_string()\n",
    "dataset_tails = dataset_df.tail().to_string()\n",
    "dataset_shape = dataset_df.shape\n",
    "rows, cols = dataset_shape\n",
    "\n",
    "# Inspect and log head, tail, dimensions and shape\n",
    "print(f\"Dataset head:\\n{dataset_head}\")\n",
    "print(f\"\\nDataset tail:\\n{dataset_tails}\")\n",
    "print(f\"\\nDataset shape: {dataset_shape}\")\n",
    "print(f\"Dataset has {cols} columns and {rows} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure all columns are shown when printing\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# Summary statistics of numeric columns\n",
    "print(\"Summary statistics:\\n\", dataset_df.describe())\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# Check for missing data\n",
    "missing_values = dataset_df.isnull().sum()\n",
    "print(f\"Missing values per column:\\n{missing_values}\")\n",
    "print(\"\")\n",
    "print(\"\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "duplicate_rows = dataset_df.duplicated().sum()\n",
    "print(f\"Number of duplicate rows: {duplicate_rows}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Type Analysis and Time Column Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data types\n",
    "print(\"Data types:\")\n",
    "print(dataset_df.dtypes)\n",
    "print(\"\\n\")\n",
    "\n",
    "# Convert Time column to datetime\n",
    "dataset_df['Time'] = pd.to_datetime(dataset_df['Time'])\n",
    "print(f\"Time column converted to datetime. Range: {dataset_df['Time'].min()} to {dataset_df['Time'].max()}\")\n",
    "\n",
    "# Check unique locations\n",
    "print(f\"\\nUnique locations: {dataset_df['item_id'].unique()}\")\n",
    "print(f\"Number of unique locations: {dataset_df['item_id'].nunique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Time-Based Visualizations for Each Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numeric columns (excluding item_id and Time)\n",
    "numeric_columns = dataset_df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "print(f\"Numeric columns to plot: {numeric_columns}\")\n",
    "\n",
    "# Get unique locations for color coding\n",
    "locations = dataset_df['item_id'].unique()\n",
    "colors = plt.cm.Set1(np.linspace(0, 1, len(locations)))\n",
    "location_colors = dict(zip(locations, colors))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create time series plots for each numeric column\n",
    "fig, axes = plt.subplots(len(numeric_columns), 1, figsize=(15, 4*len(numeric_columns)))\n",
    "if len(numeric_columns) == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot each location separately\n",
    "    for location in locations:\n",
    "        location_data = dataset_df[dataset_df['item_id'] == location]\n",
    "        ax.plot(location_data['Time'], location_data[column], \n",
    "               label=location, color=location_colors[location], \n",
    "               marker='o', markersize=3, alpha=0.7)\n",
    "    \n",
    "    ax.set_title(f'{column} over Time', fontsize=14, fontweight='bold')\n",
    "    ax.set_xlabel('Time', fontsize=12)\n",
    "    ax.set_ylabel(column, fontsize=12)\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.tick_params(axis='x', rotation=45)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save the combined plot\n",
    "plt.savefig(f'{graphs_dir}/all_variables_over_time.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {graphs_dir}/all_variables_over_time.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Individual Column Analysis with Separate Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create individual plots for better visibility\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(15, 6))\n",
    "    \n",
    "    # Plot each location\n",
    "    for location in locations:\n",
    "        location_data = dataset_df[dataset_df['item_id'] == location]\n",
    "        plt.plot(location_data['Time'], location_data[column], \n",
    "                label=location, color=location_colors[location], \n",
    "                marker='o', markersize=4, linewidth=2, alpha=0.8)\n",
    "    \n",
    "    plt.title(f'{column} over Time - Detailed View', fontsize=16, fontweight='bold')\n",
    "    plt.xlabel('Time', fontsize=14)\n",
    "    plt.ylabel(column, fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.xticks(rotation=45)\n",
    "    \n",
    "    # Add statistics text box\n",
    "    stats_text = f'Mean: {dataset_df[column].mean():.2f}\\nStd: {dataset_df[column].std():.2f}\\nMin: {dataset_df[column].min():.2f}\\nMax: {dataset_df[column].max():.2f}'\n",
    "    plt.text(0.02, 0.98, stats_text, transform=plt.gca().transAxes, \n",
    "             verticalalignment='top', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.8))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Save individual plot\n",
    "    filename = f'{graphs_dir}/{column}_over_time.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Correlation Analysis and Heatmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate correlation matrix for numeric columns\n",
    "correlation_matrix = dataset_df[numeric_columns].corr()\n",
    "\n",
    "# Create correlation heatmap\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0, \n",
    "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
    "plt.title('Correlation Matrix of Numeric Variables', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "# Save correlation heatmap\n",
    "plt.savefig(f'{graphs_dir}/correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {graphs_dir}/correlation_matrix.png\")\n",
    "plt.show()\n",
    "\n",
    "# Print strong correlations\n",
    "print(\"\\nStrong correlations (|r| > 0.7):\")\n",
    "for i in range(len(correlation_matrix.columns)):\n",
    "    for j in range(i+1, len(correlation_matrix.columns)):\n",
    "        corr_val = correlation_matrix.iloc[i, j]\n",
    "        if abs(corr_val) > 0.7:\n",
    "            print(f\"{correlation_matrix.columns[i]} vs {correlation_matrix.columns[j]}: {corr_val:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create distribution plots for each numeric column\n",
    "fig, axes = plt.subplots(2, (len(numeric_columns)+1)//2, figsize=(20, 12))\n",
    "axes = axes.flatten() if len(numeric_columns) > 1 else [axes]\n",
    "\n",
    "for i, column in enumerate(numeric_columns):\n",
    "    if i < len(axes):\n",
    "        ax = axes[i]\n",
    "        \n",
    "        # Histogram\n",
    "        ax.hist(dataset_df[column], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "        ax.set_title(f'Distribution of {column}', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel(column)\n",
    "        ax.set_ylabel('Frequency')\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_val = dataset_df[column].mean()\n",
    "        ax.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "        ax.legend()\n",
    "\n",
    "# Hide empty subplots\n",
    "for i in range(len(numeric_columns), len(axes)):\n",
    "    axes[i].set_visible(False)\n",
    "\n",
    "plt.tight_layout()\n",
    "# Save distribution plots\n",
    "plt.savefig(f'{graphs_dir}/distributions_all_variables.png', dpi=300, bbox_inches='tight')\n",
    "print(f\"Saved: {graphs_dir}/distributions_all_variables.png\")\n",
    "plt.show()\n",
    "\n",
    "# Create individual distribution plots\n",
    "for column in numeric_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Histogram\n",
    "    plt.hist(dataset_df[column], bins=30, alpha=0.7, color='skyblue', edgecolor='black')\n",
    "    plt.title(f'Distribution of {column}', fontsize=14, fontweight='bold')\n",
    "    plt.xlabel(column, fontsize=12)\n",
    "    plt.ylabel('Frequency', fontsize=12)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Add mean line\n",
    "    mean_val = dataset_df[column].mean()\n",
    "    plt.axvline(mean_val, color='red', linestyle='--', linewidth=2, label=f'Mean: {mean_val:.2f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    # Save individual distribution plot\n",
    "    filename = f'{graphs_dir}/{column}_distribution.png'\n",
    "    plt.savefig(filename, dpi=300, bbox_inches='tight')\n",
    "    print(f\"Saved: {filename}\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Summary Statistics by Location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by location and calculate summary statistics\n",
    "print(\"Summary statistics by location:\")\n",
    "for location in locations:\n",
    "    print(f\"\\n=== {location} ===\")\n",
    "    location_data = dataset_df[dataset_df['item_id'] == location]\n",
    "    print(f\"Number of records: {len(location_data)}\")\n",
    "    print(\"\\nNumeric column statistics:\")\n",
    "    print(location_data[numeric_columns].describe())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
